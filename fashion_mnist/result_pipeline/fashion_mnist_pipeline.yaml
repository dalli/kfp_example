apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: tak-test-fashion-mnist-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.13, pipelines.kubeflow.org/pipeline_compilation_time: '2022-09-15T17:18:04.938352',
    pipelines.kubeflow.org/pipeline_spec: '{"name": "tak test fashion mnist pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.13}
spec:
  entrypoint: tak-test-fashion-mnist-pipeline
  templates:
  - name: model-generation
    container:
      args: [--pretrain-model, /tmp/outputs/pretrain_model/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def model_generation(pretrain_model) :
            import tensorflow as tf
            keras_model = tf.keras.Sequential([
                tf.keras.layers.Flatten(input_shape=(28, 28)),
                tf.keras.layers.Dense(128, activation='relu'),
                tf.keras.layers.Dense(10)
            ])
            keras_model.compile(optimizer='adam',
                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                          metrics=['accuracy'])

            keras_model.save(pretrain_model)

        import argparse
        _parser = argparse.ArgumentParser(prog='Model generation', description='')
        _parser.add_argument("--pretrain-model", dest="pretrain_model", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = model_generation(**_parsed_args)
      image: tensorflow/tensorflow
    outputs:
      artifacts:
      - {name: model-generation-pretrain_model, path: /tmp/outputs/pretrain_model/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.13
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--pretrain-model", {"outputPath": "pretrain_model"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef model_generation(pretrain_model) :\n    import tensorflow
          as tf\n    keras_model = tf.keras.Sequential([\n        tf.keras.layers.Flatten(input_shape=(28,
          28)),\n        tf.keras.layers.Dense(128, activation=''relu''),\n        tf.keras.layers.Dense(10)\n    ])\n    keras_model.compile(optimizer=''adam'',\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  metrics=[''accuracy''])\n\n    keras_model.save(pretrain_model)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Model generation'', description='''')\n_parser.add_argument(\"--pretrain-model\",
          dest=\"pretrain_model\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = model_generation(**_parsed_args)\n"], "image": "tensorflow/tensorflow"}},
          "name": "Model generation", "outputs": [{"name": "pretrain_model", "type":
          "TFModel"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: model-prediction
    container:
      args: [--test-dataset, /tmp/inputs/test_dataset/data, --trained-model, /tmp/inputs/trained_model/data,
        '----output-paths', /tmp/outputs/predict/data, /tmp/outputs/label/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.4.2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def model_prediction(
            test_dataset,
            trained_model
        ):
            from tensorflow import keras
            import tensorflow as tf
            import pickle
            import pandas as pd
            import numpy as np
            import random

            class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
                           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

            with open(test_dataset, 'rb') as file:
                tr_data = pickle.load(file)

            images = []
            labels = []
            for i, item in enumerate(tr_data['image']):
                images.append(item)
                labels.append(tr_data['label'][i])
            images = np.array(images)
            labels = np.array(labels)

            test_num = random.randrange(1,1000)

            model = keras.models.load_model(trained_model)

            predic_image = images[test_num]
            predic_label = labels[test_num]

            test = tf.expand_dims(predic_image, 0)
            predictions_single = model.predict(test)
            predict_value = tf.math.argmax(tf.nn.softmax(predictions_single[0]))

            predict_value = f'predict result : {class_names[predict_value]}'
            label_value = f'label result: {class_names[predic_label]}'

            return (predict_value, label_value)

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Model prediction', description='')
        _parser.add_argument("--test-dataset", dest="test_dataset", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--trained-model", dest="trained_model", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = model_prediction(**_parsed_args)

        _output_serializers = [
            _serialize_str,
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: tensorflow/tensorflow
    inputs:
      artifacts:
      - {name: preprocess-data, path: /tmp/inputs/test_dataset/data}
      - {name: train-op-trained_model, path: /tmp/inputs/trained_model/data}
    outputs:
      parameters:
      - name: model-prediction-label
        valueFrom: {path: /tmp/outputs/label/data}
      - name: model-prediction-predict
        valueFrom: {path: /tmp/outputs/predict/data}
      artifacts:
      - {name: model-prediction-label, path: /tmp/outputs/label/data}
      - {name: model-prediction-predict, path: /tmp/outputs/predict/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.13
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--test-dataset", {"inputPath": "test_dataset"}, "--trained-model",
          {"inputPath": "trained_model"}, "----output-paths", {"outputPath": "predict"},
          {"outputPath": "label"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.4.2''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas==1.4.2'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def model_prediction(\n    test_dataset,\n    trained_model\n):\n    from
          tensorflow import keras\n    import tensorflow as tf\n    import pickle\n    import
          pandas as pd\n    import numpy as np\n    import random\n\n    class_names
          = [''T-shirt/top'', ''Trouser'', ''Pullover'', ''Dress'', ''Coat'',\n                   ''Sandal'',
          ''Shirt'', ''Sneaker'', ''Bag'', ''Ankle boot'']\n\n    with open(test_dataset,
          ''rb'') as file:\n        tr_data = pickle.load(file)\n\n    images = []\n    labels
          = []\n    for i, item in enumerate(tr_data[''image'']):\n        images.append(item)\n        labels.append(tr_data[''label''][i])\n    images
          = np.array(images)\n    labels = np.array(labels)\n\n    test_num = random.randrange(1,1000)\n\n    model
          = keras.models.load_model(trained_model)\n\n    predic_image = images[test_num]\n    predic_label
          = labels[test_num]\n\n    test = tf.expand_dims(predic_image, 0)\n    predictions_single
          = model.predict(test)\n    predict_value = tf.math.argmax(tf.nn.softmax(predictions_single[0]))\n\n    predict_value
          = f''predict result : {class_names[predict_value]}''\n    label_value =
          f''label result: {class_names[predic_label]}''\n\n    return (predict_value,
          label_value)\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(\n            str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Model
          prediction'', description='''')\n_parser.add_argument(\"--test-dataset\",
          dest=\"test_dataset\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--trained-model\",
          dest=\"trained_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = model_prediction(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "tensorflow/tensorflow"}}, "inputs": [{"name": "test_dataset",
          "type": "Dataset"}, {"name": "trained_model", "type": "TFModel"}], "name":
          "Model prediction", "outputs": [{"name": "predict", "type": "String"}, {"name":
          "label", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: preprocess
    container:
      args: [--pre-data, /tmp/inputs/pre_data/data, --data, /tmp/outputs/data/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'numpy==1.23.2' 'pandas==1.4.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'numpy==1.23.2' 'pandas==1.4.2'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def preprocess(
                pre_data,
                data
        ):
            import numpy as np
            import pickle
            import pandas as pd

            images = []
            labels = []
            with open(pre_data, 'rb') as file:
                tr_data = pickle.load(file)

            for i, item in enumerate(tr_data['image']):
                images.append(item)
                labels.append(tr_data['label'][i])
            images = np.array(images)
            labels = np.array(labels)

            images = images/255.0

            df = pd.DataFrame(columns=['image', 'label'])
            for i, image in enumerate(images):
                df.loc[i] = ({'image': image, 'label': labels[i]})

            with open(data, 'wb') as f:
                pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)

        import argparse
        _parser = argparse.ArgumentParser(prog='Preprocess', description='')
        _parser.add_argument("--pre-data", dest="pre_data", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--data", dest="data", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = preprocess(**_parsed_args)
      image: python:3.9
    inputs:
      artifacts:
      - {name: train-data-load-output_dataset_train_data, path: /tmp/inputs/pre_data/data}
    outputs:
      artifacts:
      - {name: preprocess-data, path: /tmp/outputs/data/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.13
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--pre-data", {"inputPath": "pre_data"}, "--data", {"outputPath":
          "data"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''numpy==1.23.2'' ''pandas==1.4.2''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''numpy==1.23.2'' ''pandas==1.4.2'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef preprocess(\n        pre_data,\n        data\n):\n    import
          numpy as np\n    import pickle\n    import pandas as pd\n\n    images =
          []\n    labels = []\n    with open(pre_data, ''rb'') as file:\n        tr_data
          = pickle.load(file)\n\n    for i, item in enumerate(tr_data[''image'']):\n        images.append(item)\n        labels.append(tr_data[''label''][i])\n    images
          = np.array(images)\n    labels = np.array(labels)\n\n    images = images/255.0\n\n    df
          = pd.DataFrame(columns=[''image'', ''label''])\n    for i, image in enumerate(images):\n        df.loc[i]
          = ({''image'': image, ''label'': labels[i]})\n\n    with open(data, ''wb'')
          as f:\n        pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Preprocess'', description='''')\n_parser.add_argument(\"--pre-data\",
          dest=\"pre_data\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data\",
          dest=\"data\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = preprocess(**_parsed_args)\n"],
          "image": "python:3.9"}}, "inputs": [{"name": "pre_data", "type": "Dataset"}],
          "name": "Preprocess", "outputs": [{"name": "data", "type": "Dataset"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: print-text
    container:
      args: [--text1, '{{inputs.parameters.model-prediction-predict}}', --text2, '{{inputs.parameters.model-prediction-label}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def print_text(text1, text2):
            print(text1)
            print(text2)

        import argparse
        _parser = argparse.ArgumentParser(prog='Print text', description='')
        _parser.add_argument("--text1", dest="text1", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--text2", dest="text2", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = print_text(**_parsed_args)
      image: python:3.7
    inputs:
      parameters:
      - {name: model-prediction-label}
      - {name: model-prediction-predict}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.13
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--text1", {"inputValue": "text1"}, "--text2", {"inputValue":
          "text2"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          print_text(text1, text2):\n    print(text1)\n    print(text2)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Print text'', description='''')\n_parser.add_argument(\"--text1\",
          dest=\"text1\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--text2\",
          dest=\"text2\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = print_text(**_parsed_args)\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "text1", "type": "String"},
          {"name": "text2", "type": "String"}], "name": "Print text"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"text1": "{{inputs.parameters.model-prediction-predict}}",
          "text2": "{{inputs.parameters.model-prediction-label}}"}'}
  - name: tak-test-fashion-mnist-pipeline
    dag:
      tasks:
      - {name: model-generation, template: model-generation}
      - name: model-prediction
        template: model-prediction
        dependencies: [preprocess, train-op]
        arguments:
          artifacts:
          - {name: preprocess-data, from: '{{tasks.preprocess.outputs.artifacts.preprocess-data}}'}
          - {name: train-op-trained_model, from: '{{tasks.train-op.outputs.artifacts.train-op-trained_model}}'}
      - name: preprocess
        template: preprocess
        dependencies: [train-data-load]
        arguments:
          artifacts:
          - {name: train-data-load-output_dataset_train_data, from: '{{tasks.train-data-load.outputs.artifacts.train-data-load-output_dataset_train_data}}'}
      - name: print-text
        template: print-text
        dependencies: [model-prediction]
        arguments:
          parameters:
          - {name: model-prediction-label, value: '{{tasks.model-prediction.outputs.parameters.model-prediction-label}}'}
          - {name: model-prediction-predict, value: '{{tasks.model-prediction.outputs.parameters.model-prediction-predict}}'}
      - {name: train-data-load, template: train-data-load}
      - name: train-op
        template: train-op
        dependencies: [model-generation, preprocess]
        arguments:
          artifacts:
          - {name: model-generation-pretrain_model, from: '{{tasks.model-generation.outputs.artifacts.model-generation-pretrain_model}}'}
          - {name: preprocess-data, from: '{{tasks.preprocess.outputs.artifacts.preprocess-data}}'}
  - name: train-data-load
    container:
      args: [--output-dataset-train-data, /tmp/outputs/output_dataset_train_data/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.4.2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_data_load(
                output_dataset_train_data
        ):
            import tensorflow as tf
            import pandas as pd
            import pickle

            fashion_mnist = tf.keras.datasets.fashion_mnist
            (train_images, train_labels), (_, _) = fashion_mnist.load_data()

            df = pd.DataFrame(columns=['image', 'label'])
            for i, image in enumerate(train_images):
                df.loc[i] = ({'image': image, 'label': train_labels[i]})

            with open(output_dataset_train_data, 'wb') as f:
                pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)

        import argparse
        _parser = argparse.ArgumentParser(prog='Train data load', description='')
        _parser.add_argument("--output-dataset-train-data", dest="output_dataset_train_data", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_data_load(**_parsed_args)
      image: tensorflow/tensorflow
    outputs:
      artifacts:
      - {name: train-data-load-output_dataset_train_data, path: /tmp/outputs/output_dataset_train_data/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.13
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--output-dataset-train-data", {"outputPath": "output_dataset_train_data"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.4.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.4.2''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_data_load(\n        output_dataset_train_data\n):\n    import
          tensorflow as tf\n    import pandas as pd\n    import pickle\n\n    fashion_mnist
          = tf.keras.datasets.fashion_mnist\n    (train_images, train_labels), (_,
          _) = fashion_mnist.load_data()\n\n    df = pd.DataFrame(columns=[''image'',
          ''label''])\n    for i, image in enumerate(train_images):\n        df.loc[i]
          = ({''image'': image, ''label'': train_labels[i]})\n\n    with open(output_dataset_train_data,
          ''wb'') as f:\n        pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Train data load'', description='''')\n_parser.add_argument(\"--output-dataset-train-data\",
          dest=\"output_dataset_train_data\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_data_load(**_parsed_args)\n"], "image": "tensorflow/tensorflow"}},
          "name": "Train data load", "outputs": [{"name": "output_dataset_train_data",
          "type": "Dataset"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: train-op
    container:
      args: [--train-dataset, /tmp/inputs/train_dataset/data, --pre-model, /tmp/inputs/pre_model/data,
        --trained-model, /tmp/outputs/trained_model/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.4.2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_op(
                train_dataset,
                pre_model,
                trained_model
        ) :
            import pickle
            import tensorflow as tf
            from tensorflow import keras
            import numpy as np
            import pandas as pd

            with open(train_dataset, 'rb') as file:
                tr_data = pickle.load(file)

            images = []
            labels = []
            for i, item in enumerate(tr_data['image']) :
                images.append(item)
                labels.append(tr_data['label'][i])
            images = np.array(images)
            labels = np.array(labels)

            model = keras.models.load_model(pre_model)

            model.fit(images, labels, epochs=20)

            model.save(trained_model)

        import argparse
        _parser = argparse.ArgumentParser(prog='Train op', description='')
        _parser.add_argument("--train-dataset", dest="train_dataset", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--pre-model", dest="pre_model", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--trained-model", dest="trained_model", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_op(**_parsed_args)
      image: tensorflow/tensorflow
    inputs:
      artifacts:
      - {name: model-generation-pretrain_model, path: /tmp/inputs/pre_model/data}
      - {name: preprocess-data, path: /tmp/inputs/train_dataset/data}
    outputs:
      artifacts:
      - {name: train-op-trained_model, path: /tmp/outputs/trained_model/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.13
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--train-dataset", {"inputPath": "train_dataset"}, "--pre-model",
          {"inputPath": "pre_model"}, "--trained-model", {"outputPath": "trained_model"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.4.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.4.2''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_op(\n        train_dataset,\n        pre_model,\n        trained_model\n)
          :\n    import pickle\n    import tensorflow as tf\n    from tensorflow import
          keras\n    import numpy as np\n    import pandas as pd\n\n    with open(train_dataset,
          ''rb'') as file:\n        tr_data = pickle.load(file)\n\n    images = []\n    labels
          = []\n    for i, item in enumerate(tr_data[''image'']) :\n        images.append(item)\n        labels.append(tr_data[''label''][i])\n    images
          = np.array(images)\n    labels = np.array(labels)\n\n    model = keras.models.load_model(pre_model)\n\n    model.fit(images,
          labels, epochs=20)\n\n    model.save(trained_model)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train op'', description='''')\n_parser.add_argument(\"--train-dataset\",
          dest=\"train_dataset\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pre-model\",
          dest=\"pre_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--trained-model\",
          dest=\"trained_model\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_op(**_parsed_args)\n"], "image": "tensorflow/tensorflow"}}, "inputs":
          [{"name": "train_dataset", "type": "Dataset"}, {"name": "pre_model", "type":
          "TFModel"}], "name": "Train op", "outputs": [{"name": "trained_model", "type":
          "TFModel"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
