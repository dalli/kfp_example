{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a1b803",
   "metadata": {},
   "source": [
    "# 파이프라인 (데이터 전달)\n",
    "\n",
    "파이프라인 구성에 있어서 데이터 전달은 필수적인 부분입니다.\n",
    "\n",
    "kubeflow pipeline에서 파이프라인 작성자는 직접 생성한 인스턴스(task)를 연결하여 파이프라인을 구성합니다. \n",
    "이때 인스턴스의 출력을 다른 인스턴스의 입력에 인수로 전달하여 인스턴스 간의 연결을 구성합니다.\n",
    "데이터의 크기 및 종류에 따라 시스템은 데이터를 저장하고 파이프라인 흐름에 따라 인스턴스에 전달합니다.\n",
    "\n",
    "기본적으로 작은 크기의 데이터 전달에 대한 구성을 살펴본 후 학습 데이터 같은 큰 데이터 전달에 대한 예제를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c772f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import components, dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kfp.components import func_to_container_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde02d58",
   "metadata": {},
   "source": [
    "## 간단한 데이터 전달\n",
    "\n",
    "파이프라인 구성 시 간단한 인수를 전달하여 구성할 수 있습니다.\n",
    "이때 간단한 인수는 몇 킬로바이트를 초과하지 않은 작은 데이터입니다.list, dict, JSON 형태도 적은 양의 데이터라면 간단한 인수 전달로 사용할 수 있습니다. 데이터 인수는 전달할 때 string 형태로 변환합니다.\n",
    "\n",
    "내장 변환기가 있어 일반적으로 str, int, float, bool, list, dict 형태를 지원하며 다른 형태의 데이터는 직접 변환해야 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be3748",
   "metadata": {},
   "source": [
    "### 출력 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f786d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def print_small_text(text: str):\n",
    "    '''Print small text'''\n",
    "    print(text)\n",
    "    \n",
    "@dsl.pipeline(name='hello world example')\n",
    "def test_example_pipeline():\n",
    "    '''Pipeline that passes small constant string to to consumer'''\n",
    "    print_task = print_small_text('Hello world') # Passing constant as argument to consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae1720",
   "metadata": {},
   "source": [
    "간단히 인수를 받으면 출력하는 컴포넌트와 파이프라인을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aca622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(test_example_pipeline, 'hello_world.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff93df69",
   "metadata": {},
   "source": [
    "구성 후 yaml 및 tar.gz 형태로 파이프라인 구성 파일을 생성합니다.\n",
    "생성된 파일은 kubeflow UI에서 파이프라인 생성을 통해 업로드 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedc3a7b",
   "metadata": {},
   "source": [
    "### 인수 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41130a3b",
   "metadata": {},
   "source": [
    "인수 전달 시 데이터 형태가 정의되어야 하며 여러 인자를 입력 및 출력할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def one_small_output_str() -> str:\n",
    "    return 'one_output'\n",
    "\n",
    "@func_to_container_op\n",
    "def one_small_output_int() -> int:\n",
    "    return 111\n",
    "\n",
    "@func_to_container_op\n",
    "def two_small_outputs() -> NamedTuple('Outputs', [('text', str), ('number', int)]):\n",
    "    return (\"two_output\", 222)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44814b15",
   "metadata": {},
   "source": [
    "출력 데이터 형태를 정의하여 컴포넌트 반환값을 입력합니다. 기존 출력 컴포넌트는 str 형태만을 출력하기 때문에 int 형태의 반환 값을 확인하기 어렵습니다. 아래와 같이 int 형태 출력 컴포넌트도 추가해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def print_small_int(text: int):\n",
    "    '''Print small text'''\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='data passing')\n",
    "def data_passing_pipeline():\n",
    "    '''Pipeline that passes small constant string to to consumer'''\n",
    "    single_small_output1_task = one_small_output_str()\n",
    "    single_small_output2_task = one_small_output_int()\n",
    "\n",
    "    tuple_small_output_task = two_small_outputs()\n",
    "\n",
    "    print_one1_task = print_small_text(single_small_output1_task.output)\n",
    "    print_one2_task = print_small_int(single_small_output2_task.output)\n",
    "\n",
    "    print_two1_task = print_two_arguments(tuple_small_output_task.outputs['text'], tuple_small_output_task.outputs['number'])\n",
    "    print_two1_task = print_two_arguments(single_small_output1_task.output, single_small_output2_task.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e1bbc",
   "metadata": {},
   "source": [
    "컴포넌트를 모두 정의하였다면 파이프라인을 구성합니다. 컴포넌트의 반환값은 직접 작성해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c31db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(data_passing_pipeline, 'data_passing.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78af9826",
   "metadata": {},
   "source": [
    "## 파일 전달\n",
    "\n",
    "kubeflow를 통해 머신 러닝을 사용하기 위해서는 큰 용량의 데이터 전달은 필수적입니다.\n",
    "파이프라인을 구성할 때 큰 데이터는 파일로 저장하고 불러옵니다. 이때 InputPath와 OutputPath를 활용합니다.\n",
    "파일 경로를 전달하여 대용량 데이터를 저장하고 읽어옵니다.\n",
    "\n",
    "InputPath 및 OutputPath는 type 인수를 지정하여 데이터의 유형을 지정할 수 있습니다. 본 예제에서는 간단한 Fashion-MNIST 데이터셋을 활용하기 때문에 Dataset을 활용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e1a06",
   "metadata": {},
   "source": [
    "### Fashion-MNIST 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e718515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_load(\n",
    "        output_dataset_train_data: OutputPath('Dataset')\n",
    "):\n",
    "    import tensorflow as tf\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (train_images, train_labels), (_, _) = fashion_mnist.load_data()\n",
    "\n",
    "    df = pd.DataFrame(columns=['image', 'label'])\n",
    "    for i, image in enumerate(train_images):\n",
    "        df.loc[i] = ({'image': image, 'label': train_labels[i]})\n",
    "\n",
    "    with open(output_dataset_train_data, 'wb') as f:\n",
    "        pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce84ba6",
   "metadata": {},
   "source": [
    "학습 데이터를 불러오는 컴포넌트를 생성하기 위한 함수를 작성합니다. 컴포넌트는 컨테이너로 존재하기 때문에 필요한 패키지는 함수 안에 작성합니다.\n",
    "pandas를 활용하여 dataframe 형태로 데이터를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f5c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_load_op = components.create_component_from_func(\n",
    "    train_data_load, base_image='tensorflow/tensorflow',\n",
    "    packages_to_install=['pandas==1.4.2']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bdb6e",
   "metadata": {},
   "source": [
    "컴포넌트를 생성할 때 직접 작성했던 패키지 목록과 기본이 되는 이미지를 작성합니다. 본 예제에서는 tensorflow를 활용하기 때문에 tensorflow 이미지를 불러옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1628107e",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d48e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "        pre_data:InputPath('Dataset'),\n",
    "        data: OutputPath('Dataset')\n",
    "):\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    with open(pre_data, 'rb') as file:\n",
    "        tr_data = pickle.load(file)\n",
    "\n",
    "    for i, item in enumerate(tr_data['image']):\n",
    "        images.append(item)\n",
    "        labels.append(tr_data['label'][i])\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    images = images/255.0\n",
    "\n",
    "    df = pd.DataFrame(columns=['image', 'label'])\n",
    "    for i, image in enumerate(images):\n",
    "        df.loc[i] = ({'image': image, 'label': labels[i]})\n",
    "\n",
    "    with open(data, 'wb') as f:\n",
    "        pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "preprocess_op = components.create_component_from_func(\n",
    "    preprocess, base_image='python:3.9',\n",
    "    packages_to_install=['numpy==1.23.2', 'pandas==1.4.2']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e1d02",
   "metadata": {},
   "source": [
    "이미지 데이터를 normalization 하는 컴포넌트를 생성합니다. 실제 파일 입출력처럼 경로에 있는 파일을 읽어와 데이터를 처리하고 파일로 다시 저장하는 코드를 직접 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61397bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='data load example')\n",
    "def fashion_mnist_pipeline():\n",
    "    train_data_load_task = train_data_load_op()\n",
    "    preprocess_task = preprocess_op(\n",
    "        train_data_load_task.outputs['output_dataset_train_data']\n",
    "    )\n",
    "    \n",
    "kfp.compiler.Compiler().compile(fashion_mnist_pipeline, 'fashion_mnist_data_load.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0782c1c",
   "metadata": {},
   "source": [
    "인스턴스를 생성하여 인수 전달과 같이 파이프라인을 구성한 후 실행합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
